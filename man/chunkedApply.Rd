% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utils.R
\name{chunkedApply}
\alias{chunkedApply}
\title{Reads chunks of data from a memory-mapped file into memory and applies a
function on each row or column of a matrix in parallel.}
\usage{
chunkedApply(X, MARGIN, FUN, bufferSize, nTasks = parallel::detectCores(),
  mc.cores = parallel::detectCores(), verbose = FALSE, ...)
}
\arguments{
\item{X}{A matrix-like object, typically \code{@geno} of a
\code{\link[=BGData-class]{BGData}} object.}

\item{MARGIN}{The subscripts which the function will be applied over. 1
indicates rows, 2 indicates columns.}

\item{FUN}{The function to be applied.}

\item{bufferSize}{The number of rows or columns of \code{X} that are brought
into memory for processing.}

\item{nTasks}{The number of submatrices of each buffered subset of \code{X}
to be processed in parallel.}

\item{mc.cores}{The number of cores (passed to
\code{\link[parallel]{mclapply}}).}

\item{verbose}{Whether to print additional information.}

\item{...}{Additional arguments to be passed to \code{parallelApply}.}
}
\description{
\code{bufferSize} and \code{nTasks} have to be chosen carefully to avoid
running out of memory. As a rule of thumb, at least around
\code{object_size(buffer) + (mc.cores * (object_size(buffer) / nTasks)) +
object_size(result)} MB of total memory will be needed, not including
potential copies of your data that might be created (for example \code{lsfit}
runs \code{cbind(1, X)}). Therefore, for 20 nodes and 20 tasks you will need
at least \code{2 * object_size(buffer)} MB, for 20 nodes and 40 tasks
\code{1.5 * object_size(buffer)} MB, etc.
}
\details{
This function is only useful for memory-mapped files. For data that is
already in memory, use \code{\link{parallelApply}} directly.
}

